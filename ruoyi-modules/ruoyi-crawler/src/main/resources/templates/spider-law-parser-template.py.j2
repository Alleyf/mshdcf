import datetime
import json
import traceback

from feapder import Request, BaseParser
from feapder.db.mysqldb import MysqlDB
from feapder.utils.log import log
from selenium import webdriver
from scrapy.http import HtmlResponse
from selenium.webdriver.common.by import By  #By模块
from selenium.common.exceptions import TimeoutException, NoSuchElementException  #异常模块
from selenium.webdriver.support.wait import WebDriverWait  #等待模块
from selenium.webdriver.support import expected_conditions as EC  #顶期条件的模块
from scrapy.spidermiddlewares.httperror import HttpError
from twisted.internet.error import DNSLookupError
from twisted.internet.error import TimeoutError, TCPTimedOutError

from items.DocCaseItem import DocCaseItem
from items.LawRegulationItem import LawRegulationItem
from userPool import OpenLawUserPool
from utils.decrypt import decrypt
from utils.snowflake import MySnowFlake
from utils.openLawUtil import OpenLaw
from setting import DEFAULT_USERAGENT
from utils.trimUnicode import clean_text


class {{ alias }}RegulationParser(BaseParser):
    """{{ source_name }}法律法规爬虫解析器(法条只能访问前100页)"""
    db = MysqlDB()
    # current_page = 1
    start_page = 1  # 起始页码
    stop_page = 1  # 终止页码
    is_limit = False  # 是否限制爬取页数
    total_page = 100  # 总页数
    current_item = 1  # 当前爬取条目数
    start_urls = {{ start_urls }}
    test_url = "{{ source_url }}"
    # 自定义配置(日志···)
    __custom_setting__ = dict(
        # 请求超时随请求数变化（一个请求默认20s，一页的请求个数为20个请求，乘以总页数得到总请求超时时间）
        REQUEST_TIMEOUT=20 * 20 * total_page,
    )

    def __init__(self):
        self.openlaw = OpenLaw()  #实例化openlaw类
        # self.cookies_dict = openlaw.local_cookie_login() #获取cookie
        self.cookies_dict = self.openlaw.cookie_login()  #获取cookie
        print(self.cookies_dict)

    def download_midware(self, request):
        """设置cookie将cookie添加到请求中"""
        request.cookies = self.cookies_dict
        # log.info("对所有请求添加cookie")
        return request

    def start_requests(self):
        for url in self.start_urls:
            yield Request(url=url, callback=self.parseLs)

    #   yield Request(url=self.start_urls[0],callback=self.parseLs)
    #   yield Request(url=self.test_url)
    #   yield Request(url=self.test_url, render=True, callback=self.parseLs)
    # pass

    def validate(self, request, response):
        """校验请求是否正常"""
        # 重定向到登录界面则重新登陆
        if "login" in response.url:
            # log.info(f"返回内容为：{response.text}")
            log.error(f"cookie失效,重新获取cookie,{request.url}->请求失败,抛弃该请求;实际请求为：{response.url}")
            self.openlaw.clear_cookie()  #清除cookie
            self.cookies_dict = self.openlaw.cookie_login()  #获取cookie
            raise Exception("账户登录状态失效，请重新登录")  # 抛出异常则重试
        # 状态码不为200则请求失败，抛弃该请求
        if response.status_code != 200:
            log.info(f"返回内容为：{response.text}")
            log.error(f"请求失败：{response.url}, 状态码：{response.status_code}, 被抛弃")
            return False

    # def parse(self, request, response):
    #     print(response.text)

    def parseLs(self, request, response):
        """列表页解析获取法规名称和详细页地址"""
        log.info("--------------------------------开始请求<列表页>--------------------------------")
        # log.info(response.text)
        try:
            law_ls = response.css("div#ht-kb div.ht-kb-category ul.ht-kb-article-list > li")
            # log.info(law_ls)
            for law in law_ls:
                title = law.css("a::attr(title)").extract_first()
                law_id = law.css("a::attr(href)").extract_first().split("/")[-1]
                detail_url = self.start_urls[0] + law_id
                #   yield Request(url=detail_url, cookies=self.cookies_dict, callback=self.parse_detail, title = title, errback=self.errback)
                yield Request(url=detail_url, cookies=self.cookies_dict, callback=self.parse_detail, title=title)
            # 请求下一页
            # 获取总数据条数计算总页数
            total_count = response.css(
                "nav.ht-pagination ul.page-numbers li:nth-last-child(1) label::text").extract_first()
            self.total_page = int(total_count) // 20
            if self.total_page % 20 != 0:
                self.total_page += 1  #计算总页数
            # 根据is_limit限制爬取页数
            if self.is_limit:
                self.total_page = min(self.total_page, self.stop_page)
            log.info("爬取进度：" + str(self.start_page) + "/" + str(self.total_page))
            # next_rel_urlLs = response.css("div#ht-kb > nav.ht-pagination > ul.page-numbers > a.next.page-numbers::attr(href)").extract_first().split('&')
            next_url = request.url.split("?")[0] + '?page=%d' % self.start_page  #获取下一页的url
            if next_url != None and self.start_page < self.total_page:
                self.start_page += 1  #当前页码自增
                #   yield Request(url=next_url, cookies=self.cookies_dict, callback=self.parseLs, errback=self.errback,render=True)
                yield Request(url=next_url, cookies=self.cookies_dict, callback=self.parseLs)
        except Exception as e:
            traceback.print_exc()
            log.error(f"列表页面 {response.url} 解析失败：{e}")
            raise Exception(f"列表页面 {response.url} 解析失败：{e}")  # 抛出异常则重试
            # return None

    def parse_detail(self, request, response):
        """详细页解析获取法规基本信息"""
        log.info("--------------------------------开始请求<详细页>--------------------------------")
        item = LawRegulationItem()
        try:
            item["id"] = MySnowFlake.create_id()
            item["url"] = request.url
            item["name"] = request.title
            # 确保 response 已经被正确解码为 Unicode 字符串
            # 去除正文为空的响应
            if response.text.strip() == "" or len(response.text.strip()) == 0:
                log.debug(f"请求失败：{response.url}, 正文为空：{response.text}, 被抛弃")
                return False
            if isinstance(response.text, bytes):
                log.debug(f"译码前的 {item['name']} 为{response.text}")
                response.text = response.text.decode('utf-8')
                log.debug(f"译码后的 {item['name']} 为{response.text}")
            # 然后使用解码后的文本执行 CSS 选择器操作
            # article_selector = Selector(text=response_text).css("div#ht-kb article")
            article_selector = response.css("div#ht-kb article")
            # if article_selector
            item['release_date'] = article_selector.css(
                "header ul li:nth-child(1) time::attr(datetime)").extract_first()
            item['execute_date'] = article_selector.css(
                "header ul li:nth-child(2) time::attr(datetime)").extract_first()

            contents_selector = article_selector.css("div#entry-cont")  #一直为空？？？
            # log.info(contents_selector)
            contentLs = contents_selector.css("*::text").extract()
            contentLs1 = contents_selector.css('*:not(li[style="list-style-type:none;"])')
            contentLs2 = contents_selector.css('li:has(a)')
            contentLs3 = [item for item in contentLs2 if item not in contentLs1]
            # 遍历将contengLs中所有的selector都提取text并添加到一个新的list中
            contentLsTemp = []
            for content in contentLs3:
                content = content.css("*::text").extract()
                content = "".join(content)
                contentLsTemp.append(content)
            # contentLs = contentLs.css("*::text").extract()
            # log.info(contentLs)
            contents = []
            for content in contentLs:
                if content in contentLsTemp:
                    continue
                content = content.strip()
                content = clean_text(content)
                if content != "" and content != None:
                    contents.append(content)
            item['content'] = "\n".join(contents)  #该处格式可能有错误
            other_selector = response.css("div.ht-container aside#sidebar section:nth-child(1) ul")
            # 使用正则表达式提取，避免顺序错误
            item['type'] = other_selector.re_first(regex="类型：(.*?)<", replace_entities=False)
            item['structure'] = other_selector.re_first(regex="法规结构：(.*?)<", replace_entities=False)
            item['revise_num'] = other_selector.re_first(regex="修改次数：(.*?)<", replace_entities=False)
            item['release_organization'] = other_selector.re_first(regex="发文机关：(.*?)<", replace_entities=False)
            item['is_validity'] = other_selector.re_first(regex="有效性：(.*?)<", replace_entities=False)

            item['source_id'] = 1  #来源于openlaw

            yield item

        except Exception as e:
            traceback.print_exc()
            log.error(f"详细页面 {response.url} 解析失败：{e}")
            raise Exception(f"详细页面 {response.url} 解析失败：{e}")  # 抛出异常则重试
            # return None

    def errback(self, failure):
        #   log.error(repr(failure))

        #   if failure.check(HttpError):
        #         response = failure.value.response
        #         log.error("HttpError on %s" % response.url)

        #   elif failure.check(DNSLookupError):
        #         request = failure.request
        #         log.error("DNSLookupError on %s" % request.url)

        #   elif failure.check(TimeoutError, TCPTimedOutError):
        #         request = failure.request
        #         log.error("TimeoutError on %s" % request.url)
        return False
